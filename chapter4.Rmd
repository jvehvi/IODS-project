---
title: "chapter4.Rmd"
output: html_document
date: "2022-11-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 4: Clustering and classification

This assignment topic is clustering and classification. As basic, clustering means that in the data some points/observations are in space so near each other compared to other points/observations that those form a 'own cluster'.
There are multiple different clustering methods to find those clusters from the data. One of the most common methods is k-means clustering, others worth naming are hierarchical clustering methods which gives dendrograms as a output.
If the clustering can be done successfully, new observations can be tried to classify to those clusters.

To clarify this topic, Boston dataset from MASS - package will be used as a demonstration dataset.

The Boston Dataset

The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The Boston data frame has 506 rows and 14 columns. The following describes the dataset columns:

*    CRIM - per capita crime rate by town
*    ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
*    INDUS - proportion of non-retail business acres per town.
*    CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
*    NOX - nitric oxides concentration (parts per 10 million)
*    RM - average number of rooms per dwelling
*   AGE - proportion of owner-occupied units built prior to 1940
*   DIS - weighted distances to five Boston employment centres*
*    RAD - index of accessibility to radial highways
*    TAX - full-value property-tax rate per $10,000
*    PTRATIO - pupil-teacher ratio by town
*    B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
*    LSTAT - % lower status of the population
 *   MEDV - Median value of owner-occupied homes in $1000's

## Summary and graphical overview of the data
```{r, echo=FALSE}
# Bring needed libraries to R-environment
library(MASS)
library(ggplot2)
library(GGally)
library(reshape2)
library(tidyr)
library(corrplot)

# Check head of Boston table
head(Boston)

# Check dimensions
dim(Boston)

# Check summary of data
summary(Boston)

# Visualize data for inspecting
## Scatterplot
ggpairs(Boston, mapping = aes(), lower =list(combo = wrap("facethist", bins = 20))) 

## All variables against crim rate
bosmelt <- melt(Boston, id="crim")
ggplot(bosmelt, aes(x=value, y=crim))+
  facet_wrap(~variable, scales="free")+
  geom_point()

# Correlation matrix and visualization
## Matrix
corrmatrix <- cor(Boston, use = "complete.obs")
corrmatrix
## Visualization
corrplot(corrmatrix, method="circle")


```

## Standardize the dataset

You can also embed plots, for example:

```{r}
# Check head of original data to compare scaled data
head(Boston)
# center and standardize variables, also set output to be data.frame
boston_scaled <- as.data.frame(scale(Boston, center = TRUE, scale = TRUE))
head(boston_scaled)
# Set 'crim' to numeric variable
boston_scaled$crim <- as.numeric(boston_scaled$crim)
# Summary of crim variable
summary(boston_scaled[,'crim'])

# class of the boston_scaled object
class(boston_scaled)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled[['crim']])
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled[['crim']], breaks = bins, include.lowest = TRUE)

# look at the table of the new factor crime
head(boston_scaled)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# Set 'crime' to factor variable, and give relevant levels
boston_scaled$crime <- factor(boston_scaled$crime, levels = c("low", "med_low", "med_high", "high"))

```

## Divide the dataset to train and test sets
Data will be divided 80/20 to train and test sets

```{r}
# Bring needed libraries to R-environment
library(dplyr)
library(MASS)

# Bring Boston data table from github to R
boston_scaled <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/boston_scaled.txt", sep=",", header = T)

# Change 'crime' to factor variable with relevant levels
boston_scaled$crime <- factor(boston_scaled$crime, levels = c("low", "med_low", "med_high", "high"))

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```



## Fit the linear discriminant analysis on the train se

* Crime rate as the target variable and all the other variables in the dataset as predictor variables.
```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2) +
  lda.arrows(lda.fit, myscale = 1)


```

## Predict the classes with the LDA model on the test data

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

## Reload the Boston dataset and standardize the dataset 

* Scale all variables to have mean = 0 and standard deviation = 1
```{r}
# Bring data to R
data(Boston)
# Standaridise data
standardised_boston <- as.data.frame(Boston %>%mutate_all(~(scale(.) %>% as.vector)))

# Calculate distances

## Euclidean distance matrix
dist_eu <- dist(standardised_boston, method = "euclidean")

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(standardised_boston, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

set.seed(13)

# K-means clustering, center 4
km <- kmeans(Boston, centers = 4)
# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

# K-means clustering, center 3
km <- kmeans(Boston, centers = 3)
# plot the Boston dataset with clusters
ggpairs(Boston[1:5], col = km$cluster)
ggpairs(Boston[6:10], col = km$cluster)
ggpairs(Boston[11:ncol(Boston)], col = km$cluster)


# K-means clustering, center 2
km <- kmeans(Boston, centers = 2)
# plot the Boston dataset with clusters
ggpairs(Boston[1:5], col = km$cluster)
ggpairs(Boston[6:10], col = km$cluster)
ggpairs(Boston[11:ncol(Boston)], col = km$cluster)

# K-means clustering, center 1
km <- kmeans(Boston, centers = 1)
# plot the Boston dataset with clusters
ggpairs(Boston[1:5], col = km$cluster)
ggpairs(Boston[6:10], col = km$cluster)
ggpairs(Boston[11:ncol(Boston)], col = km$cluster)
```

Distances in Manhattan method are longer. Two clusters seems to be best.

## Bonus: k-means on the original Boston data with 5 clusters
